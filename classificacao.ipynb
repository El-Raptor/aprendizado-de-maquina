{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/Documentos/2020.1/IA/trab2/entrega/py36/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Nome - RGA:\n",
    "    Fábio Holanda Saraiva Júnior - 2015.1905.006-2\n",
    "    Felipe Salles Lopez - 2016.1907.032-4\n",
    "    Lucas Avanzi - 2016.1907.024-3\n",
    "    Lucas Antonio dos Santos - 2016.1907.013-8\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "dataset_path = [\n",
    "                \"./datasets/arrhythmia/arrhythmia.data\",\n",
    "                \"./datasets/balance-scale/balance-scale.data\",\n",
    "                \"./datasets/blood-transfusion/transfusion.data\",\n",
    "                \"./datasets/echocardiogram/echocardiogram.data\",\n",
    "                \"./datasets/glass/glass.data\",\n",
    "                \"./datasets/haberman/haberman.data\",\n",
    "                \"./datasets/hepatitis/hepatitis.data\",\n",
    "                \"./datasets/iris/iris.data\",\n",
    "                \"./datasets/lymphography/lymphography.data\",\n",
    "                \"./datasets/tae/tae.data\"\n",
    "                ]\n",
    "first_class_line = [\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "]\n",
    "\n",
    "class_index = [\n",
    "    279,\n",
    "    0,\n",
    "    4,\n",
    "    12,\n",
    "    10,\n",
    "    3,\n",
    "    0,\n",
    "    4,\n",
    "    0,\n",
    "    5\n",
    "]\n",
    "\n",
    "def read_input_file(file_path):\n",
    "    open_file = open(file_path, \"r\")\n",
    "    reader = csv.reader(open_file)\n",
    "    data = list(reader)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base de dados de arritmia cardíaca\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_0 = read_input_file(dataset_path[0])\n",
    "\n",
    "#len(dataset_0[0])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base de dados de peso e distância da balança\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_1 = read_input_file(dataset_path[1])\n",
    "\n",
    "#dataset_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dados de centro de serviços de transfusão de sangue\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_2 = read_input_file(dataset_path[2])\n",
    "\n",
    "#dataset_2[0]\n",
    "#dataset_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dados de ecocardiograma\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_3 = read_input_file(dataset_path[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Banco de dados de identificação de vidro\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_4 = read_input_file(dataset_path[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Banco de dados de Haberman\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_5 = read_input_file(dataset_path[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Banco de dados hepatite\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_6 = read_input_file(dataset_path[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.30522282\n",
      "Iteration 2, loss = 1.29872744\n",
      "Iteration 3, loss = 1.28975798\n",
      "Iteration 4, loss = 1.27884640\n",
      "Iteration 5, loss = 1.26650598\n",
      "Iteration 6, loss = 1.25321397\n",
      "Iteration 7, loss = 1.23939974\n",
      "Iteration 8, loss = 1.22543799\n",
      "Iteration 9, loss = 1.21164612\n",
      "Iteration 10, loss = 1.19828489\n",
      "Iteration 11, loss = 1.18556114\n",
      "Iteration 12, loss = 1.17363172\n",
      "Iteration 13, loss = 1.16260763\n",
      "Iteration 14, loss = 1.15255824\n",
      "Iteration 15, loss = 1.14351539\n",
      "Iteration 16, loss = 1.13547762\n",
      "Iteration 17, loss = 1.12841458\n",
      "Iteration 18, loss = 1.12227192\n",
      "Iteration 19, loss = 1.11697632\n",
      "Iteration 20, loss = 1.11244087\n",
      "Iteration 21, loss = 1.10857013\n",
      "Iteration 22, loss = 1.10526498\n",
      "Iteration 23, loss = 1.10242684\n",
      "Iteration 24, loss = 1.09996123\n",
      "Iteration 25, loss = 1.09778047\n",
      "Iteration 26, loss = 1.09580570\n",
      "Iteration 27, loss = 1.09396806\n",
      "Iteration 28, loss = 1.09220931\n",
      "Iteration 29, loss = 1.09048183\n",
      "Iteration 30, loss = 1.08874819\n",
      "Iteration 31, loss = 1.08698046\n",
      "Iteration 32, loss = 1.08515918\n",
      "Iteration 33, loss = 1.08327234\n",
      "Iteration 34, loss = 1.08131418\n",
      "Iteration 35, loss = 1.07928410\n",
      "Iteration 36, loss = 1.07718558\n",
      "Iteration 37, loss = 1.07502520\n",
      "Iteration 38, loss = 1.07281175\n",
      "Iteration 39, loss = 1.07055550\n",
      "Iteration 40, loss = 1.06826748\n",
      "Iteration 41, loss = 1.06595895\n",
      "Iteration 42, loss = 1.06364090\n",
      "Iteration 43, loss = 1.06132363\n",
      "Iteration 44, loss = 1.05901640\n",
      "Iteration 45, loss = 1.05672712\n",
      "Iteration 46, loss = 1.05446216\n",
      "Iteration 47, loss = 1.05222612\n",
      "Iteration 48, loss = 1.05002179\n",
      "Iteration 49, loss = 1.04785013\n",
      "Iteration 50, loss = 1.04571030\n",
      "Iteration 51, loss = 1.04359984\n",
      "Iteration 52, loss = 1.04151485\n",
      "Iteration 53, loss = 1.03945028\n",
      "Iteration 54, loss = 1.03740022\n",
      "Iteration 55, loss = 1.03535822\n",
      "Iteration 56, loss = 1.03331760\n",
      "Iteration 57, loss = 1.03127174\n",
      "Iteration 58, loss = 1.02921434\n",
      "Iteration 59, loss = 1.02713966\n",
      "Iteration 60, loss = 1.02504263\n",
      "Iteration 61, loss = 1.02291899\n",
      "Iteration 62, loss = 1.02076532\n",
      "Iteration 63, loss = 1.01857906\n",
      "Iteration 64, loss = 1.01635849\n",
      "Iteration 65, loss = 1.01410260\n",
      "Iteration 66, loss = 1.01181108\n",
      "Iteration 67, loss = 1.00948413\n",
      "Iteration 68, loss = 1.00712238\n",
      "Iteration 69, loss = 1.00472677\n",
      "Iteration 70, loss = 1.00229842\n",
      "Iteration 71, loss = 0.99983853\n",
      "Iteration 72, loss = 0.99734829\n",
      "Iteration 73, loss = 0.99482880\n",
      "Iteration 74, loss = 0.99228104\n",
      "Iteration 75, loss = 0.98970581\n",
      "Iteration 76, loss = 0.98710371\n",
      "Iteration 77, loss = 0.98447516\n",
      "Iteration 78, loss = 0.98182042\n",
      "Iteration 79, loss = 0.97913961\n",
      "Iteration 80, loss = 0.97643272\n",
      "Iteration 81, loss = 0.97369970\n",
      "Iteration 82, loss = 0.97094048\n",
      "Iteration 83, loss = 0.96815499\n",
      "Iteration 84, loss = 0.96534325\n",
      "Iteration 85, loss = 0.96250532\n",
      "Iteration 86, loss = 0.95964142\n",
      "Iteration 87, loss = 0.95675184\n",
      "Iteration 88, loss = 0.95383702\n",
      "Iteration 89, loss = 0.95089754\n",
      "Iteration 90, loss = 0.94793407\n",
      "Iteration 91, loss = 0.94494738\n",
      "Iteration 92, loss = 0.94193834\n",
      "Iteration 93, loss = 0.93890788\n",
      "Iteration 94, loss = 0.93585697\n",
      "Iteration 95, loss = 0.93278661\n",
      "Iteration 96, loss = 0.92969784\n",
      "Iteration 97, loss = 0.92659167\n",
      "Iteration 98, loss = 0.92346912\n",
      "Iteration 99, loss = 0.92033120\n",
      "Iteration 100, loss = 0.91717890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/Documentos/2020.1/IA/trab2/entrega/py36/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Banco de dados iris\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "#dataset_7 = read_input_file(dataset_path[7])\n",
    "pd_dataset_7 = pd.read_csv(dataset_path[7], header = None)\n",
    "array_7 = np.array(pd_dataset_7)\n",
    "\n",
    "\n",
    "#Obter apenas os dados e gerar a lista de dados:\n",
    "#array_7[:, 0:4].tolist()\n",
    "\n",
    "#Obter classes e gerar listas de classe:\n",
    "#array_7[:, 4].tolist()\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (5,), activation=\"logistic\", max_iter = 100, alpha = 0.001, solver = \"sgd\",\n",
    "                   tol = 1e-9, learning_rate_init=.01, verbose = True)\n",
    "\n",
    "data = array_7[:, 0:4]\n",
    "target = array_7[:, 4]\n",
    "\n",
    "mlp = mlp.fit (data, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset Domínio Linfográfico\n",
    "\n",
    "Citação: \n",
    "This lymphography domain was obtained from the University Medical Centre\n",
    "Institute of Oncology, Ljubljana, Yugoslavia.  Thanks go to M. Zwitter and \n",
    "   M. Soklic for providing the data.\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_8 = read_input_file(dataset_path[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset Avaliação Assistente de Ensino\n",
    "\"\"\"\n",
    "\n",
    "#leitura do dataset\n",
    "dataset_9 = read_input_file(dataset_path[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
